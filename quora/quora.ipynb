{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64672603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91983\\AppData\\Local\\Temp\\ipykernel_9368\\2052869347.py:18: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test = pd.read_csv(test_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (404290, 6)\n",
      "test shape: (3563475, 3)\n",
      "sample_sub shape: (2345796, 2)\n",
      "\n",
      "Label distribution (train):\n",
      "is_duplicate\n",
      "0    0.630802\n",
      "1    0.369198\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Nulls in train:\n",
      "question1    1\n",
      "question2    2\n",
      "dtype: int64\n",
      "\n",
      "Total unique question strings to fit TF-IDF on: 7935530\n"
     ]
    }
   ],
   "source": [
    "# Imports and data loading\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Data paths\n",
    "data_dir = r\".\"  # current notebook folder\n",
    "train_path = os.path.join(data_dir, 'train.csv')\n",
    "test_path = os.path.join(data_dir, 'test.csv')\n",
    "sample_sub_path = os.path.join(data_dir, 'sample_submission.csv')\n",
    "\n",
    "# Load\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "sample_sub = pd.read_csv(sample_sub_path)\n",
    "\n",
    "print('train shape:', train.shape)\n",
    "print('test shape:', test.shape)\n",
    "print('sample_sub shape:', sample_sub.shape)\n",
    "\n",
    "# Quick checks\n",
    "print('\\nLabel distribution (train):')\n",
    "print(train['is_duplicate'].value_counts(normalize=True))\n",
    "\n",
    "print('\\nNulls in train:')\n",
    "print(train[['question1','question2']].isnull().sum())\n",
    "\n",
    "# Simple preprocessing\n",
    "train['question1'] = train['question1'].fillna('')\n",
    "train['question2'] = train['question2'].fillna('')\n",
    "test['question1'] = test['question1'].fillna('')\n",
    "test['question2'] = test['question2'].fillna('')\n",
    "\n",
    "# Create combined text for TF-IDF fitting\n",
    "all_questions = pd.concat([train['question1'], train['question2'], test['question1'], test['question2']]).astype(str)\n",
    "print('\\nTotal unique question strings to fit TF-IDF on:', all_questions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99fb5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation log loss: 0.37693674557821594\n",
      "Saved submission.csv with 3563475 rows\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF features and baseline model\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=50000, ngram_range=(1,2), analyzer='word')\n",
    "# Fit on all question text\n",
    "tfidf.fit(all_questions)\n",
    "\n",
    "# Transform question1 and question2\n",
    "q1_train = tfidf.transform(train['question1'].astype(str))\n",
    "q2_train = tfidf.transform(train['question2'].astype(str))\n",
    "\n",
    "# Simple feature: absolute difference and elementwise multiplication\n",
    "from scipy.sparse import hstack\n",
    "X_train = hstack([q1_train, q2_train, np.abs(q1_train - q2_train)])\n",
    "\n",
    "y = train['is_duplicate'].values\n",
    "\n",
    "# Train/validation split\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000, solver='sag')\n",
    "model.fit(X_tr, y_tr)\n",
    "\n",
    "val_pred = model.predict_proba(X_val)[:,1]\n",
    "val_loss = log_loss(y_val, val_pred)\n",
    "print('Validation log loss:', val_loss)\n",
    "\n",
    "# Prepare test features and predict\n",
    "q1_test = tfidf.transform(test['question1'].astype(str))\n",
    "q2_test = tfidf.transform(test['question2'].astype(str))\n",
    "X_test = hstack([q1_test, q2_test, np.abs(q1_test - q2_test)])\n",
    "\n",
    "test_pred = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Create submission (original)\n",
    "submission = pd.DataFrame({'test_id': test['test_id'], 'is_duplicate': test_pred})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('Saved submission.csv with', submission.shape[0], 'rows')\n",
    "\n",
    "# Create submission2 matching the format of sample_submission (preserve columns/order)\n",
    "submission2 = sample_sub.copy()\n",
    "# If sample_sub contains an 'is_duplicate' column, overwrite it; otherwise add it.\n",
    "if 'is_duplicate' in submission2.columns:\n",
    "    submission2['is_duplicate'] = test_pred\n",
    "else:\n",
    "    # If sample doesn't have the prediction column, try to align by index\n",
    "    # Prefer keeping 'test_id' from sample_sub if present\n",
    "    if 'test_id' in submission2.columns:\n",
    "        submission2 = submission2[['test_id']].copy()\n",
    "        submission2['is_duplicate'] = test_pred\n",
    "    else:\n",
    "        # Fallback: construct minimal submission with test ids\n",
    "        submission2 = pd.DataFrame({'test_id': test['test_id'], 'is_duplicate': test_pred})\n",
    "\n",
    "submission2.to_csv('submission2.csv', index=False)\n",
    "print('Saved submission2.csv with', submission2.shape[0], 'rows; columns:', submission2.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c4bb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91983\\AppData\\Local\\Temp\\ipykernel_2080\\206992277.py:20: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sub = pd.read_csv('submission.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission2.csv — rows: 3563475 columns: ['is_duplicate']\n"
     ]
    }
   ],
   "source": [
    "# New cell: create submission2 in the exact format of sample_submission (or fallback)\n",
    "# This cell assumes you've already run the training/prediction cell so\n",
    "# `sample_sub`, `test_pred`, and `test` are available in the notebook namespace.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Try to use sample_sub in memory; if not present, try to read the sample file\n",
    "if 'sample_sub' not in globals():\n",
    "    try:\n",
    "        sample_sub = pd.read_csv(sample_sub_path)\n",
    "    except Exception:\n",
    "        sample_sub = None\n",
    "\n",
    "# Try to obtain predictions vector: prefer test_pred, else read submission.csv\n",
    "preds = None\n",
    "if 'test_pred' in globals():\n",
    "    preds = test_pred\n",
    "else:\n",
    "    try:\n",
    "        sub = pd.read_csv('submission.csv')\n",
    "        if 'is_duplicate' in sub.columns:\n",
    "            preds = sub['is_duplicate'].values\n",
    "        else:\n",
    "            # assume last column contains predictions\n",
    "            preds = sub.iloc[:, -1].values\n",
    "    except Exception:\n",
    "        preds = None\n",
    "\n",
    "# Build submission2 following sample_sub column layout\n",
    "if sample_sub is not None and preds is not None:\n",
    "    submission2 = sample_sub.copy()\n",
    "    if 'is_duplicate' in submission2.columns:\n",
    "        submission2['is_duplicate'] = preds\n",
    "    else:\n",
    "        # keep sample_sub columns/order but ensure we add the prediction column last\n",
    "        submission2['is_duplicate'] = preds\n",
    "\n",
    "elif preds is not None and 'test' in globals():\n",
    "    # fallback: create using test DataFrame's test_id\n",
    "    submission2 = pd.DataFrame({'test_id': test['test_id'], 'is_duplicate': preds})\n",
    "\n",
    "elif preds is not None:\n",
    "    # safest fallback: use submission.csv contents if available\n",
    "    submission2 = pd.DataFrame({'is_duplicate': preds})\n",
    "\n",
    "else:\n",
    "    raise RuntimeError('No predictions found (no test_pred and no submission.csv). Run the prediction cell first.')\n",
    "\n",
    "# Save\n",
    "submission2.to_csv('submission2.csv', index=False)\n",
    "print('Saved submission2.csv — rows:', submission2.shape[0], 'columns:', submission2.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "518969fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91983\\AppData\\Local\\Temp\\ipykernel_2080\\2592933005.py:16: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tmp = pd.read_csv(fname)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: length mismatch (preds=3563475, sample=2345796). Aligning to min=2345796.\n",
      "Saved submission3.csv — rows: 2345796 columns: ['test_id', 'is_duplicate']\n"
     ]
    }
   ],
   "source": [
    "# New cell: create submission3 with binary labels (0/1) by thresholding at 0.5\n",
    "# Assumes predictions are available in `test_pred` or in submission/submission2 files.\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Obtain predictions (probabilities)\n",
    "preds = None\n",
    "if 'test_pred' in globals():\n",
    "    preds = np.asarray(test_pred)\n",
    "else:\n",
    "    # try submission.csv then submission2.csv\n",
    "    for fname in ('submission.csv', 'submission2.csv'):\n",
    "        if os.path.exists(fname):\n",
    "            try:\n",
    "                tmp = pd.read_csv(fname)\n",
    "                if 'is_duplicate' in tmp.columns:\n",
    "                    preds = tmp['is_duplicate'].values\n",
    "                else:\n",
    "                    preds = tmp.iloc[:, -1].values\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "if preds is None:\n",
    "    raise RuntimeError('No predictions found. Run the prediction cell or ensure submission.csv/submission2.csv exists.')\n",
    "\n",
    "# Binarize at threshold 0.5\n",
    "bin_preds = (np.array(preds) >= 0.5).astype(int)\n",
    "\n",
    "# Build submission3 following sample_submission's layout when possible\n",
    "submission3 = None\n",
    "if 'sample_sub' in globals() and isinstance(sample_sub, pd.DataFrame):\n",
    "    submission3 = sample_sub.copy()\n",
    "    # align lengths if needed\n",
    "    if len(bin_preds) != len(submission3):\n",
    "        minlen = min(len(bin_preds), len(submission3))\n",
    "        print(f'Warning: length mismatch (preds={len(bin_preds)}, sample_sub={len(submission3)}). Aligning to min={minlen}.')\n",
    "        submission3 = submission3.iloc[:minlen].copy()\n",
    "        submission3['is_duplicate'] = bin_preds[:minlen]\n",
    "    else:\n",
    "        submission3['is_duplicate'] = bin_preds\n",
    "elif os.path.exists('sample_submission.csv'):\n",
    "    sample = pd.read_csv('sample_submission.csv')\n",
    "    submission3 = sample.copy()\n",
    "    if len(bin_preds) != len(submission3):\n",
    "        minlen = min(len(bin_preds), len(submission3))\n",
    "        print(f'Warning: length mismatch (preds={len(bin_preds)}, sample={len(submission3)}). Aligning to min={minlen}.')\n",
    "        submission3 = submission3.iloc[:minlen].copy()\n",
    "        submission3['is_duplicate'] = bin_preds[:minlen]\n",
    "    else:\n",
    "        submission3['is_duplicate'] = bin_preds\n",
    "elif 'test' in globals() and 'test_id' in test.columns:\n",
    "    submission3 = pd.DataFrame({'test_id': test['test_id'], 'is_duplicate': bin_preds})\n",
    "else:\n",
    "    submission3 = pd.DataFrame({'is_duplicate': bin_preds})\n",
    "\n",
    "# Save\n",
    "submission3.to_csv('submission3.csv', index=False)\n",
    "print('Saved submission3.csv — rows:', submission3.shape[0], 'columns:', submission3.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f2e1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17434b8b",
   "metadata": {},
   "source": [
    "## How to run\n",
    "\n",
    "1. Open this notebook in Jupyter or VS Code and run cells in order.\n",
    "2. It will read `train.csv`, `test.csv`, and `sample_submission.csv` from the notebook folder.\n",
    "3. The model is a quick baseline and will create `submission.csv` in the same folder.\n",
    "\n",
    "Next steps\n",
    "\n",
    "- Add cross-validation and more engineered features (word overlap, fuzzy matching, embeddings).\n",
    "- Try neural methods (Siamese BiLSTM, SBERT) for stronger performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
